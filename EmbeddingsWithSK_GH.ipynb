{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install semantic-kernel -U\n",
    "#! pip install qdrant_client -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "from semantic_kernel.connectors.memory.qdrant import QdrantMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_service_id = \"default\"\n",
    "\n",
    "embed_service_id=\"embedding\"\n",
    "\n",
    "\n",
    "oai_chat_service = AzureChatCompletion(\n",
    "        service_id=chat_service_id,\n",
    "        endpoint=\"\",\n",
    "        api_key=\"\",\n",
    "        deployment_name = \"gpt-4o\",\n",
    "        api_version = \"2024-08-01-preview\",\n",
    "        #kernel = kernel\n",
    ")\n",
    " \n",
    "\n",
    "embedding_gen = AzureTextEmbedding(\n",
    "        service_id=embed_service_id,\n",
    "        deployment_name = \"text-embedding-ada-002\", \n",
    "        endpoint = \"\",\n",
    "        api_key=\"\",\n",
    "        api_version = \"2023-05-15\",\n",
    "        #kernel = kernel\n",
    ")\n",
    "        \n",
    "kernel.add_service(oai_chat_service)\n",
    "kernel.add_service(embedding_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_store = QdrantMemoryStore(vector_size=1536, url=\"http://localhost\",port=6333)\n",
    "await qdrant_store.create_collection('aboutMe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='TextMemoryPlugin', description=None, functions={'recall': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='recall', plugin_name='TextMemoryPlugin', description='Recall a fact from the long term memory', parameters=[KernelParameterMetadata(name='ask', description='The information to retrieve', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The information to retrieve'}, include_in_function_choices=True), KernelParameterMetadata(name='collection', description='The collection to search for information.', default_value='generic', type_='str', is_required=False, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The collection to search for information.'}, include_in_function_choices=True), KernelParameterMetadata(name='relevance', description='The relevance score, from 0.0 to 1.0; 1.0 means perfect match', default_value=0.75, type_='float', is_required=False, type_object=<class 'float'>, schema_data={'type': 'number', 'description': 'The relevance score, from 0.0 to 1.0; 1.0 means perfect match'}, include_in_function_choices=True), KernelParameterMetadata(name='limit', description='The maximum number of relevant memories to recall.', default_value=1, type_='int', is_required=False, type_object=<class 'int'>, schema_data={'type': 'integer', 'description': 'The maximum number of relevant memories to recall.'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x00000275FF5212D0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x00000275FF5E4490>, method=<bound method TextMemoryPlugin.recall of TextMemoryPlugin(memory=SemanticTextMemory(), embeddings_kwargs={})>, stream_method=None), 'save': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='save', plugin_name='TextMemoryPlugin', description='Save information to semantic memory', parameters=[KernelParameterMetadata(name='text', description='The information to save.', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The information to save.'}, include_in_function_choices=True), KernelParameterMetadata(name='key', description='The unique key to associate with the information.', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The unique key to associate with the information.'}, include_in_function_choices=True), KernelParameterMetadata(name='collection', description='The collection to save the information.', default_value='generic', type_='str', is_required=False, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The collection to save the information.'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='None', is_required=False, type_object=None, schema_data={'type': 'object'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x00000275893B4ED0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000027588D1B590>, method=<bound method TextMemoryPlugin.save of TextMemoryPlugin(memory=SemanticTextMemory(), embeddings_kwargs={})>, stream_method=None)})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = SemanticTextMemory(storage=qdrant_store, embeddings_generator=embedding_gen)\n",
    "kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_vectordb = 'aboutMe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def populate_memory(memory: SemanticTextMemory) -> None:\n",
    "    # Add some documents to the semantic memory\n",
    "    await memory.save_information(base_vectordb, id=\"info1\", text=\"Jack is Microsoft Cloud Advocate\")\n",
    "    await memory.save_information(base_vectordb, id=\"info2\", text=\"Jack is ex-IBM employee\")\n",
    "    await memory.save_information(base_vectordb, id=\"info3\", text=\"Jack is AI/ML Expert\")\n",
    "    await memory.save_information(base_vectordb, id=\"info4\", text=\"OpenAI is a company that is developing artificial general intelligence (AGI) with widely distributed economic benefits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "await populate_memory(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask = \"who is Jackï¼Ÿ\"\n",
    "\n",
    "memories = await memory.search(base_vectordb, ask, limit=2, min_relevance_score=0.77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Result: Jack is ex-IBM employee with score 0.85357594\n",
      "Top 2 Result: Jack is AI/ML Expert with score 0.8478449\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for memory in memories:\n",
    "    i = i + 1\n",
    "    print(f\"Top {i} Result: {memory.text} with score {memory.relevance}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
